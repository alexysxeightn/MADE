# Машинное обучение в продакшене. ДЗ 2

[Условия](./hw2.md)

### Сборка Docker-контейнера:

```
docker build -t IMAGE_NAME .
docker run -p 8000:8000 -it IMAGE_NAME
```

### Сборка контейнера из dockerhub:

```
docker pull alexysxeightn/ml_in_prod_hw2:v2
docker run -p 8000:8000 -it alexysxeightn/ml_in_prod_hw2:v2
```

### Запуск скрипта, который обращается к сервису:

```
python3 script.py
```

### Тестирование сервиса:

```
pytest tests/test_app.py
```

### Описание API:

`/` - корневой endpoint, (`GET`)

`/health` - проверяет, что модель и трансформер загрузились (`GET`)

`/predict` - делает предсказание по данным, подробно можно узнать в `/docs` (`POST`)

### Эксперименты по уменьшению объема Docker-образа:

Изначально в `Dockerfile` использовал python3.8, объем образа был [561 Мб](https://hub.docker.com/layers/ml_in_prod_hw2/alexysxeightn/ml_in_prod_hw2/v1/images/sha256-03425966e0c3adb2a559aea8b24b64355a42edffb187f51587b736180fa08983?context=explore), после замены на python:3.7-slim-stretch объем образа уменьшился до [247 Мб](https://hub.docker.com/layers/ml_in_prod_hw2/alexysxeightn/ml_in_prod_hw2/v2/images/sha256-b11e51822404642af72e3b2dbd4e43c5f0b5b5bca78dc64a090a3480b115903c?context=explore). Изначально файл писал так, чтобы минимизировать число слоёв и чтобы самые часто изменяемые строки находились ниже.


### Баллы:

#### Основная часть: 
1. Оберните inference вашей модели в rest сервис на FastAPI, должен быть endpoint /predict (3/3)
2. Напишите endpoint /health, должен возращать 200, если ваша модель готова к работе (такой чек особенно актуален если делаете доп задание про скачивание из хранилища) (1/1)
3. Напишите unit тест для /predict (3/3)
4. Напишите скрипт, который будет делать запросы к вашему сервису (2/2)
5. Напишите dockerfile, соберите на его основе образ и запустите локально контейнер(docker build, docker run), внутри контейнера должен запускать сервис, написанный в предущем пункте, закоммитьте его, напишите в readme корректную команду сборки (4/4)
5. Опубликуйте образ в https://hub.docker.com/, используя docker push (вам потребуется зарегистрироваться) (2/2)
6. напишите в readme корректные команды docker pull/run, которые должны привести к тому, что локально поднимется на inference ваша модель. Убедитесь, что вы можете протыкать его скриптом из пункта 3 (1/1)
7. Проведите самооценку (распишите в реквесте какие пункты выполнили и на сколько баллов, укажите сумму баллов) (1/1)

#### Дополнительная часть
1. Ваш сервис скачивает модель из S3 или любого другого хранилища при старте, путь для скачивания передается через переменные окружения (0/2)
2. Оптимизируйте размер docker image (опишите в readme.md что вы предприняли для сокращения размера и каких результатов удалось добиться -- должно получиться мини исследование, я сделал тото и получился такой-то результат) (2/2)
3. Сделайте валидацию входных данных (например, порядок колонок не совпадает с трейном, типы, допустимые максимальные и минимальные значения, проявите фантазию, это доп. баллы, проверка не должна быть тривиальной) (вы можете сохранить вместе с моделью доп информацию, о структуре входных данных, если это нужно). Возращайте 400, в случае, если валидация не пройдена (0/2)

Суммарные баллы: 19/23.
